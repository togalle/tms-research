{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /opt/conda/lib/python3.11/site-packages (1.7.1)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (6.8.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from mne) (3.1.4)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /opt/conda/lib/python3.11/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from mne) (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from mne) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from mne) (24.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /opt/conda/lib/python3.11/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.7.1 in /opt/conda/lib/python3.11/site-packages (from mne) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from mne) (4.66.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (2.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from pooch>=1.5->mne) (4.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from pooch>=1.5->mne) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->mne) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mne colorlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "\n",
    "logger = utils.get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "The functions below calculate and visually represent the importance of features in a given dataframe, useful for analysis of the importances in the different analysis paradigms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_attributions(filenames, source_folder=\"features-4\", top_n=20):\n",
    "    logger.info(f\"Getting PCA attributions of {len(filenames)} files\")\n",
    "    \n",
    "    logger.info(f\"Loading {len(filenames)} files\")\n",
    "    dfs = [\n",
    "        pd.read_csv(os.path.join(source_folder, file), header=[0, 1])\n",
    "        for file in filenames\n",
    "    ]\n",
    "    df = pd.concat(dfs)\n",
    "    feature_names = df.columns\n",
    "\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "    \n",
    "    # PCA\n",
    "    logger.info(\"Executing PCA\")\n",
    "    pca = PCA(n_components=3)\n",
    "    df_reduced = pca.fit_transform(df)\n",
    "    \n",
    "    # Feature Loadings\n",
    "    logger.info(\"Getting loadings\")\n",
    "    loadings = pca.components_\n",
    "    top_features = {}\n",
    "    for i, component in enumerate(loadings):\n",
    "        component_loadings = zip(feature_names, component)\n",
    "        sorted_loadings = sorted(component_loadings, key=lambda x: abs(x[1]), reverse=True)\n",
    "        top_features[i] = sorted_loadings[:top_n]\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "def plot_hierarchical_importances(data):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, (component, features) in enumerate(data.items()):\n",
    "        feature_names = [f\"{feature[0]}\" for feature, _ in features]\n",
    "        importances = [importance for _, importance in features]\n",
    "        \n",
    "        sns.barplot(x=importances, y=feature_names, ax=axes[i])\n",
    "        axes[i].set_title(f'Principal Component {i+1}')\n",
    "        axes[i].set_xlabel('Importance')\n",
    "        axes[i].set_ylabel('Feature')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def combine_and_aggregate(data):\n",
    "    combined_importances = defaultdict(float)\n",
    "    \n",
    "    for component in data.values():\n",
    "        for (feature, importance) in component:\n",
    "            combined_importances[feature] += abs(importance)\n",
    "    \n",
    "    combined_importances = sorted(combined_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return combined_importances\n",
    "\n",
    "def plot_combined_importances(combined_importances):\n",
    "    top_features = combined_importances\n",
    "    \n",
    "    feature_names = [f\"{feature[0]}\" for feature, _ in top_features]\n",
    "    importances = [importance for _, importance in top_features]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=importances, y=feature_names)\n",
    "    plt.title('Top Features by Combined Importance')\n",
    "    plt.xlabel('Aggregated Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def top_feature_importance(filenames, source_folder=\"features-4\", top_n=20):\n",
    "    top_features = get_pca_attributions(filenames, source_folder, top_n)\n",
    "    plot_hierarchical_importances(top_features)\n",
    "    combined_importances = combine_and_aggregate(top_features)\n",
    "    plot_combined_importances(combined_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "When the features have been extracted, there's now a list of files containing one entry per epoch. Each file has a certain paradigm containing: patient, procedure, timing. Clustering will be applied for each individual procedure containing all timings (pre & post). These clusters will be generated once with all patients and once per individual patient.\n",
    "\n",
    "The desired outcome is two clusters per plot; one for data before the procedure and one for after the procedure (with the active control procedure being the exception since this should have no impact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(df, n_clusters=2, plot_title=\"PCA of Clusters\", algorithm='kmeans'):\n",
    "    \"\"\"Expects one large dataframe containing all the data to be clustered, and one column 'timings' containing the label for each entry to compare cluster result against ground truth.\"\"\"\n",
    "    if \"label\" not in df.columns:\n",
    "        logger.error(\"No timings column found in DataFrame\")\n",
    "        return\n",
    "\n",
    "    df_ground_truth = df\n",
    "    df = df.drop(columns=[\"label\"])\n",
    "    feature_names = df.columns\n",
    "\n",
    "    # Outlier removal\n",
    "    OUTLIER_THRESHOLD = 0.05\n",
    "    Q1 = df.quantile(0.10)\n",
    "    Q3 = df.quantile(0.90)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    def is_outlier(row):\n",
    "        return ((row < (Q1 - 1.5 * IQR)) | (row > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "    outlier_counts = df.apply(is_outlier, axis=1)\n",
    "    threshold = len(df.columns) * OUTLIER_THRESHOLD\n",
    "    rows_to_drop = outlier_counts[outlier_counts > threshold].index\n",
    "    df_filtered = df.drop(index=rows_to_drop)\n",
    "    df_ground_truth = df_ground_truth.drop(index=rows_to_drop)\n",
    "    print(f\"Original DataFrame shape: {df.shape}\")\n",
    "    print(f\"Filtered DataFrame shape: {df_filtered.shape}\")\n",
    "    df = df_filtered\n",
    "\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    # Apply clustering algorithm\n",
    "    if algorithm == 'kmeans':\n",
    "        model = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    elif algorithm == 'gmm':\n",
    "        model = GaussianMixture(n_components=n_clusters, n_init=10)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported algorithm. Use 'kmeans' or 'gmm'.\")\n",
    "\n",
    "    clusters = model.fit_predict(df)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    df_reduced = pca.fit_transform(df)\n",
    "\n",
    "    # 2D Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(df_reduced[:, 0], df_reduced[:, 1], c=clusters, cmap=\"viridis\")\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(\n",
    "        df_reduced[:, 0],\n",
    "        df_reduced[:, 1],\n",
    "        c=df_ground_truth[\"label\"].astype(\"category\").cat.codes,\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "    plt.title(\"KMeans Clustering Results - Ground Truth\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Ground Truth Label\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # 3D Plot\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.scatter(df_reduced[:, 0], df_reduced[:, 1], df_reduced[:, 2], c=clusters, cmap=\"viridis\")\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.scatter(df_reduced[:, 0], df_reduced[:, 1], df_reduced[:, 2], c=df_ground_truth[\"label\"].astype(\"category\").cat.codes, cmap=\"viridis\")\n",
    "    ax.set_title(\"KMeans Clustering Results - Ground Truth\")\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Attributions\n",
    "    loadings = pca.components_\n",
    "    for i, component in enumerate(loadings):\n",
    "        component_loadings = zip(feature_names, component)\n",
    "        sorted_loadings = sorted(component_loadings, key=lambda x: abs(x[1]), reverse=True)\n",
    "        print(f\"Principal Component {i+1}:\")\n",
    "        for feature, loading in sorted_loadings[:5]:\n",
    "            print(f\"{feature}: {loading}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>procedure</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>eeg_type</th>\n",
       "      <th>pre_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TMS-EEG-H_07_S3_rsEEG_pre-epo.csv</td>\n",
       "      <td>itbs</td>\n",
       "      <td>07</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TMS-EEG-H_06_S2_rsEEG_pre-epo.csv</td>\n",
       "      <td>itbs</td>\n",
       "      <td>06</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TMS-EEG-H_14_S1_rsEEG_pre-epo.csv</td>\n",
       "      <td>ctbs</td>\n",
       "      <td>14</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TMS-EEG-H_17_S2_rsEEG_pre-epo.csv</td>\n",
       "      <td>sham</td>\n",
       "      <td>17</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TMS-EEG-H_16_S3_rsEEG_pre-epo.csv</td>\n",
       "      <td>itbs</td>\n",
       "      <td>16</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>TMS-EEG-H_02_S3_rsEEG_pre-epo.csv</td>\n",
       "      <td>ctbs</td>\n",
       "      <td>02</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>TMS-EEG-H_09_S3_rsEEG_post-epo.csv</td>\n",
       "      <td>sham</td>\n",
       "      <td>09</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>TMS-EEG-H_08_s1_rsEEG_pre-epo.csv</td>\n",
       "      <td>sham</td>\n",
       "      <td>08</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>TMS-EEG-H_16_S2_rsEEG_post-epo.csv</td>\n",
       "      <td>sham</td>\n",
       "      <td>16</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>TMS-EEG-H_10_S2_rsEEG_post-epo.csv</td>\n",
       "      <td>itbs</td>\n",
       "      <td>10</td>\n",
       "      <td>rsEEG</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename procedure patient_id eeg_type pre_post\n",
       "0    TMS-EEG-H_07_S3_rsEEG_pre-epo.csv      itbs         07    rsEEG      pre\n",
       "1    TMS-EEG-H_06_S2_rsEEG_pre-epo.csv      itbs         06    rsEEG      pre\n",
       "2    TMS-EEG-H_14_S1_rsEEG_pre-epo.csv      ctbs         14    rsEEG      pre\n",
       "3    TMS-EEG-H_17_S2_rsEEG_pre-epo.csv      sham         17    rsEEG      pre\n",
       "4    TMS-EEG-H_16_S3_rsEEG_pre-epo.csv      itbs         16    rsEEG      pre\n",
       "..                                 ...       ...        ...      ...      ...\n",
       "83   TMS-EEG-H_02_S3_rsEEG_pre-epo.csv      ctbs         02    rsEEG      pre\n",
       "84  TMS-EEG-H_09_S3_rsEEG_post-epo.csv      sham         09    rsEEG     post\n",
       "85   TMS-EEG-H_08_s1_rsEEG_pre-epo.csv      sham         08    rsEEG      pre\n",
       "86  TMS-EEG-H_16_S2_rsEEG_post-epo.csv      sham         16    rsEEG     post\n",
       "87  TMS-EEG-H_10_S2_rsEEG_post-epo.csv      itbs         10    rsEEG     post\n",
       "\n",
       "[88 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = utils.get_metadata_df(\"features-4\", \"Randomisatielijst.csv\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all data for feature importance to find out the most general trend first\n",
    "filenames = labels[\n",
    "    (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "][\"filename\"]\n",
    "\n",
    "top_feature_importance(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session variability\n",
    "By clustering all pre sessions per patient, we can get a feel for the inter-session variability. If this is too high, it means that the distinction in feature values is primarily based upon this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for patientid in range(2, 19):\n",
    "    filenames = labels[\n",
    "        (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "        & (labels[\"patient_id\"] == f\"{patientid:02}\")\n",
    "        & (labels[\"pre_post\"] == \"pre\")\n",
    "    ][\"filename\"]\n",
    "\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "\n",
    "    top_feature_importance(filenames)\n",
    "\n",
    "    # Get corresponding labels\n",
    "    procedure_labels = [\n",
    "        labels[labels[\"filename\"] == file][\"procedure\"].values[0]\n",
    "        for file in filenames\n",
    "    ]\n",
    "\n",
    "    # Load dataframe\n",
    "    dfs = [\n",
    "        pd.read_csv(os.path.join(\"features-4\", file), header=[0, 1])\n",
    "        for file in filenames\n",
    "    ]\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Add procedure column\n",
    "    ground_truth = []\n",
    "    for procedure, df_part in zip(procedure_labels, dfs):\n",
    "        ground_truth.extend([procedure] * len(df_part))\n",
    "    df[\"label\"] = ground_truth\n",
    "\n",
    "    cluster_df(df, n_clusters=3, plot_title=f\"Procedure clustering patient {patientid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual procedures\n",
    "\n",
    "The code below applies the clustering function to dataframes for each **individual procedure**. First, it's applied to all patients and then to each individual patient. We expect two seperate clusters for itbs and ctbs: one before and one after procedure, marking the impact of the procedure. Sham shouldn't have any impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for procedure in [\"itbs\", \"ctbs\", \"sham\"]:\n",
    "    logger.info(f\"Clustering {procedure}\")\n",
    "    # Cluster all patients\n",
    "    filenames = labels[\n",
    "        (labels[\"procedure\"] == procedure)\n",
    "        & (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "    ][\"filename\"]\n",
    "\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "\n",
    "    top_feature_importance(filenames)\n",
    "\n",
    "    # Get corresponding labels\n",
    "    pre_post_labels = [\n",
    "        labels[labels[\"filename\"] == file][\"pre_post\"].values[0]\n",
    "        for file in filenames\n",
    "    ]\n",
    "\n",
    "    # Load dataframe\n",
    "    dfs = [\n",
    "        pd.read_csv(os.path.join(\"features-4\", file), header=[0, 1])\n",
    "        for file in filenames\n",
    "    ]\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Add timings column\n",
    "    ground_truth = []\n",
    "    for timing, df_part in zip(pre_post_labels, dfs):\n",
    "        ground_truth.extend([timing] * len(df_part))\n",
    "    df[\"label\"] = ground_truth\n",
    "\n",
    "    # cluster_df(df, plot_title=procedure, algorithm=\"gmm\")\n",
    "    cluster_df(df, plot_title=procedure)\n",
    "\n",
    "    # Cluster individual patients\n",
    "    for patientid in range(2, 19):\n",
    "        # Get relevant filenames\n",
    "        filenames = labels[\n",
    "            (labels[\"procedure\"] == procedure)\n",
    "            & (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "            & (labels[\"patient_id\"] == f\"{patientid:02}\")\n",
    "        ][\"filename\"]\n",
    "\n",
    "        if len(filenames) == 0:\n",
    "            continue\n",
    "\n",
    "        top_feature_importance(filenames)\n",
    "\n",
    "        # Get corresponding labels\n",
    "        pre_post_labels = [\n",
    "            labels[labels[\"filename\"] == file][\"pre_post\"].values[0]\n",
    "            for file in filenames\n",
    "        ]\n",
    "\n",
    "        # Load dataframe\n",
    "        dfs = [\n",
    "            pd.read_csv(os.path.join(\"features-4\", file), header=[0, 1])\n",
    "            for file in filenames\n",
    "        ]\n",
    "        df = pd.concat(dfs)\n",
    "\n",
    "        # Add timings column\n",
    "        ground_truth = []\n",
    "        for timing, df_part in zip(pre_post_labels, dfs):\n",
    "            ground_truth.extend([timing] * len(df_part))\n",
    "        df[\"label\"] = ground_truth\n",
    "\n",
    "        # cluster_df(df, plot_title=procedure, algorithm=\"gmm\")\n",
    "        cluster_df(df, plot_title=procedure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post procedures\n",
    "\n",
    "The code below applies the clustering function to dataframes after **all procedures** (timing is **post**). First, it's applied to all patients and then to each individual patient. We expect three seperate clusters for each procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Clustering all post procedure data\")\n",
    "\n",
    "# Cluster all patients\n",
    "filenames = labels[\n",
    "    (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "    & (labels[\"pre_post\"] == \"post\")\n",
    "][\"filename\"]\n",
    "\n",
    "top_feature_importance(filenames)\n",
    "\n",
    "# Get corresponding labels\n",
    "procedure_labels = [\n",
    "    labels[labels[\"filename\"] == file][\"procedure\"].values[0]\n",
    "    for file in filenames\n",
    "]\n",
    "\n",
    "# Load dataframe\n",
    "dfs = [\n",
    "    pd.read_csv(os.path.join(\"features-4\", file), header=[0, 1])\n",
    "    for file in filenames\n",
    "]\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Add procedure column\n",
    "ground_truth = []\n",
    "for procedure, df_part in zip(procedure_labels, dfs):\n",
    "    ground_truth.extend([procedure] * len(df_part))\n",
    "df[\"label\"] = ground_truth\n",
    "\n",
    "cluster_df(df, n_clusters=3, plot_title=\"Procedure clustering\")\n",
    "\n",
    "# Cluster individual patients\n",
    "for patientid in range(2, 19):\n",
    "    # Get relevant filenames\n",
    "    filenames = labels[\n",
    "        (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "        & (labels[\"patient_id\"] == f\"{patientid:02}\")\n",
    "        & (labels[\"pre_post\"] == \"post\")\n",
    "    ][\"filename\"]\n",
    "\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "\n",
    "    top_feature_importance(filenames)\n",
    "\n",
    "    # Get corresponding labels\n",
    "    procedure_labels = [\n",
    "        labels[labels[\"filename\"] == file][\"procedure\"].values[0]\n",
    "        for file in filenames\n",
    "    ]\n",
    "\n",
    "    # Load dataframe\n",
    "    dfs = [\n",
    "        pd.read_csv(os.path.join(\"features-4\", file), header=[0, 1])\n",
    "        for file in filenames\n",
    "    ]\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Add procedure column\n",
    "    ground_truth = []\n",
    "    for procedure, df_part in zip(procedure_labels, dfs):\n",
    "        ground_truth.extend([procedure] * len(df_part))\n",
    "    df[\"label\"] = ground_truth\n",
    "\n",
    "    cluster_df(df, n_clusters=3, plot_title=f\"Procedure clustering patient {patientid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure paired t-test\n",
    "\n",
    "Another test that could be of interest is a paired t-test where for each procedure, the impact the procedure had on the average feature values on the patient is tested. This is done with a paired t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def process_and_average(pre_filenames, post_filenames, source_folder=\"features-4\"):\n",
    "    def load_data(filenames):\n",
    "        data = []\n",
    "        for filename in filenames:\n",
    "            df = pd.read_csv(os.path.join(source_folder, filename), header=[0, 1])\n",
    "            data.append(df)\n",
    "        return pd.concat(data, ignore_index=True)\n",
    "    \n",
    "    # Load pre and post data\n",
    "    pre_data = load_data(pre_filenames)\n",
    "    post_data = load_data(post_filenames)\n",
    "    \n",
    "    # Concatenate pre and post data\n",
    "    combined_data = pd.concat([pre_data, post_data], ignore_index=True)\n",
    "\n",
    "    # Normalize the combined data\n",
    "    scaler = StandardScaler()\n",
    "    normalized_combined = pd.DataFrame(scaler.fit_transform(combined_data), columns=combined_data.columns)\n",
    "    \n",
    "    # Split back into pre and post dataframes\n",
    "    normalized_pre_data = normalized_combined.iloc[:len(pre_data)]\n",
    "    normalized_post_data = normalized_combined.iloc[len(pre_data):]\n",
    "    \n",
    "    # Average the dataframes to one row\n",
    "    averaged_pre_data = normalized_pre_data.mean().to_frame().T\n",
    "    averaged_post_data = normalized_post_data.mean().to_frame().T\n",
    "    \n",
    "    return averaged_pre_data, averaged_post_data\n",
    "\n",
    "def perform_t_test(normalize=True):\n",
    "    t_test_results = {}\n",
    "\n",
    "    # t-test for each procedure individually\n",
    "    for procedure in [\"itbs\", \"ctbs\", \"sham\"]:\n",
    "        logger.info(f\"Performing t-test for {procedure}\")\n",
    "        all_pre_avg_values = []\n",
    "        all_post_avg_values = []\n",
    "\n",
    "        # get average value entries for each patient\n",
    "        for patientid in range(2, 19):\n",
    "            pre_filenames = labels[\n",
    "                (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "                & (labels[\"patient_id\"] == f\"{patientid:02}\")\n",
    "                & (labels[\"pre_post\"] == \"pre\")\n",
    "                & (labels[\"procedure\"] == procedure)\n",
    "            ][\"filename\"]\n",
    "            \n",
    "            post_filenames = labels[\n",
    "                (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "                & (labels[\"patient_id\"] == f\"{patientid:02}\")\n",
    "                & (labels[\"pre_post\"] == \"post\")\n",
    "                & (labels[\"procedure\"] == procedure)\n",
    "            ][\"filename\"]\n",
    "            \n",
    "            if len(pre_filenames) == 0 or len(post_filenames) == 0:\n",
    "                continue\n",
    "            \n",
    "            pre_avg_values, post_avg_values = process_and_average(pre_filenames, post_filenames)\n",
    "\n",
    "            all_pre_avg_values.append(pre_avg_values)\n",
    "            all_post_avg_values.append(post_avg_values)\n",
    "\n",
    "        if all_pre_avg_values and all_post_avg_values:\n",
    "            # convert list to df\n",
    "            all_pre_avg_values = pd.concat(all_pre_avg_values, ignore_index=True)\n",
    "            all_post_avg_values = pd.concat(all_post_avg_values, ignore_index=True)\n",
    "            \n",
    "            t_test_results[procedure] = {}\n",
    "            # t-test for individual features\n",
    "            for column in all_pre_avg_values.columns:\n",
    "                t_stat, p_value = ttest_rel(all_pre_avg_values[column], all_post_avg_values[column])\n",
    "                \n",
    "                t_test_results[procedure][column] = {\n",
    "                    \"t_stat\": t_stat,\n",
    "                    \"p_value\": p_value\n",
    "                }\n",
    "    return t_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-18 12:47:46,967] - INFO - Performing t-test for itbs\u001b[0m\n",
      "\u001b[32m[2024-08-18 12:49:17,055] - INFO - Performing t-test for ctbs\u001b[0m\n",
      "\u001b[32m[2024-08-18 12:50:38,484] - INFO - Performing t-test for sham\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant features for itbs:\n",
      "- variance\n",
      "- std\n",
      "- skewness\n",
      "- rms\n",
      "- hjorth_mobility\n",
      "- hjorth_complexity\n",
      "- zero_crossings\n",
      "- line_length\n",
      "- app_entropy\n",
      "- spect_entropy\n",
      "- hurst_exp\n",
      "- pow_freq_bands\n",
      "- phase_lock_val\n",
      "- time_corr\n",
      "- spect_corr\n",
      "Significant features for ctbs:\n",
      "- variance\n",
      "- std\n",
      "- ptp_amp\n",
      "- skewness\n",
      "- kurtosis\n",
      "- rms\n",
      "- hjorth_mobility\n",
      "- zero_crossings\n",
      "- app_entropy\n",
      "- pow_freq_bands\n",
      "- phase_lock_val\n",
      "- time_corr\n",
      "- spect_corr\n",
      "Significant features for sham:\n",
      "- variance\n",
      "- std\n",
      "- ptp_amp\n",
      "- skewness\n",
      "- rms\n",
      "- line_length\n",
      "- hurst_exp\n",
      "- pow_freq_bands\n",
      "- phase_lock_val\n",
      "- time_corr\n",
      "- spect_corr\n"
     ]
    }
   ],
   "source": [
    "t_test_results = perform_t_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for itbs:\n",
      "- variance: Average p-value = 0.58940\n",
      "  * Lowest p-value observed at: Fp1 (p-value = 0.02290)\n",
      "- std: Average p-value = 0.58830\n",
      "  * Lowest p-value observed at: Fp1 (p-value = 0.03359)\n",
      "- ptp_amp: Average p-value = 0.61729\n",
      "  * Lowest p-value observed at: Fp1 (p-value = 0.06698)\n",
      "- skewness: Average p-value = 0.54334\n",
      "  * Lowest p-value observed at: PO3 (p-value = 0.02991)\n",
      "- kurtosis: Average p-value = 0.62806\n",
      "  * Lowest p-value observed at: TP8 (p-value = 0.06036)\n",
      "- rms: Average p-value = 0.59975\n",
      "  * Lowest p-value observed at: Fp1 (p-value = 0.03266)\n",
      "- hjorth_mobility: Average p-value = 0.29200\n",
      "  * Lowest p-value observed at: P7 (p-value = 0.00116)\n",
      "- hjorth_complexity: Average p-value = 0.27386\n",
      "  * Lowest p-value observed at: CP1 (p-value = 0.00154)\n",
      "- zero_crossings: Average p-value = 0.25533\n",
      "  * Lowest p-value observed at: TP10 (p-value = 0.00065)\n",
      "- line_length: Average p-value = 0.38005\n",
      "  * Lowest p-value observed at: TP10 (p-value = 0.00323)\n",
      "- app_entropy: Average p-value = 0.23917\n",
      "  * Lowest p-value observed at: CP3 (p-value = 0.00113)\n",
      "- spect_entropy: Average p-value = 0.30797\n",
      "  * Lowest p-value observed at: TP10 (p-value = 0.00408)\n",
      "- hurst_exp: Average p-value = 0.61133\n",
      "  * Lowest p-value observed at: TP10 (p-value = 0.01271)\n",
      "- pow_freq_bands: Average p-value = 0.43633\n",
      "  * Lowest p-value observed at: TP10_band0 (p-value = 0.00276)\n",
      "- phase_lock_val: Average p-value = 0.46207\n",
      "  * Lowest p-value observed at: PO8-FCz (p-value = 0.00071)\n",
      "- time_corr: Average p-value = 0.49236\n",
      "  * Lowest p-value observed at: C1-Fpz (p-value = 0.00330)\n",
      "- spect_corr: Average p-value = 0.47147\n",
      "  * Lowest p-value observed at: PO3-CP5 (p-value = 0.00012)\n",
      "Features for ctbs:\n",
      "- variance: Average p-value = 0.47755\n",
      "  * Lowest p-value observed at: C1 (p-value = 0.00654)\n",
      "- std: Average p-value = 0.48018\n",
      "  * Lowest p-value observed at: C1 (p-value = 0.00671)\n",
      "- ptp_amp: Average p-value = 0.50656\n",
      "  * Lowest p-value observed at: C1 (p-value = 0.02456)\n",
      "- skewness: Average p-value = 0.45774\n",
      "  * Lowest p-value observed at: FT7 (p-value = 0.01811)\n",
      "- kurtosis: Average p-value = 0.55133\n",
      "  * Lowest p-value observed at: CPz (p-value = 0.04999)\n",
      "- rms: Average p-value = 0.49479\n",
      "  * Lowest p-value observed at: C1 (p-value = 0.01613)\n",
      "- hjorth_mobility: Average p-value = 0.64426\n",
      "  * Lowest p-value observed at: FC6 (p-value = 0.04256)\n",
      "- hjorth_complexity: Average p-value = 0.60006\n",
      "  * Lowest p-value observed at: FT7 (p-value = 0.06113)\n",
      "- zero_crossings: Average p-value = 0.66166\n",
      "  * Lowest p-value observed at: FC6 (p-value = 0.03743)\n",
      "- line_length: Average p-value = 0.64622\n",
      "  * Lowest p-value observed at: FC6 (p-value = 0.14184)\n",
      "- app_entropy: Average p-value = 0.59899\n",
      "  * Lowest p-value observed at: FC6 (p-value = 0.03114)\n",
      "- spect_entropy: Average p-value = 0.60298\n",
      "  * Lowest p-value observed at: FC6 (p-value = 0.08961)\n",
      "- hurst_exp: Average p-value = 0.72349\n",
      "  * Lowest p-value observed at: TP7 (p-value = 0.05173)\n",
      "- pow_freq_bands: Average p-value = 0.62856\n",
      "  * Lowest p-value observed at: Fp2_band2 (p-value = 0.02549)\n",
      "- phase_lock_val: Average p-value = 0.46251\n",
      "  * Lowest p-value observed at: P1-FC6 (p-value = 0.00101)\n",
      "- time_corr: Average p-value = 0.48084\n",
      "  * Lowest p-value observed at: FC1-FT7 (p-value = 0.00032)\n",
      "- spect_corr: Average p-value = 0.55765\n",
      "  * Lowest p-value observed at: P4-FC6 (p-value = 0.00045)\n",
      "Features for sham:\n",
      "- variance: Average p-value = 0.49939\n",
      "  * Lowest p-value observed at: FCz (p-value = 0.00802)\n",
      "- std: Average p-value = 0.50520\n",
      "  * Lowest p-value observed at: FCz (p-value = 0.00944)\n",
      "- ptp_amp: Average p-value = 0.52391\n",
      "  * Lowest p-value observed at: FCz (p-value = 0.02350)\n",
      "- skewness: Average p-value = 0.21406\n",
      "  * Lowest p-value observed at: PO3 (p-value = 0.00060)\n",
      "- kurtosis: Average p-value = 0.54661\n",
      "  * Lowest p-value observed at: AF4 (p-value = 0.07738)\n",
      "- rms: Average p-value = 0.48111\n",
      "  * Lowest p-value observed at: FCz (p-value = 0.00835)\n",
      "- hjorth_mobility: Average p-value = 0.64871\n",
      "  * Lowest p-value observed at: FC6 (p-value = 0.13389)\n",
      "- hjorth_complexity: Average p-value = 0.65929\n",
      "  * Lowest p-value observed at: Fp1 (p-value = 0.15450)\n",
      "- zero_crossings: Average p-value = 0.58551\n",
      "  * Lowest p-value observed at: FC6 (p-value = 0.09007)\n",
      "- line_length: Average p-value = 0.55078\n",
      "  * Lowest p-value observed at: FC6 (p-value = 0.04362)\n",
      "- app_entropy: Average p-value = 0.64605\n",
      "  * Lowest p-value observed at: Fp1 (p-value = 0.13416)\n",
      "- spect_entropy: Average p-value = 0.66466\n",
      "  * Lowest p-value observed at: FC6 (p-value = 0.09683)\n",
      "- hurst_exp: Average p-value = 0.45374\n",
      "  * Lowest p-value observed at: CP2 (p-value = 0.02631)\n",
      "- pow_freq_bands: Average p-value = 0.52000\n",
      "  * Lowest p-value observed at: F1_band1 (p-value = 0.03015)\n",
      "- phase_lock_val: Average p-value = 0.49024\n",
      "  * Lowest p-value observed at: P5-FC6 (p-value = 0.00623)\n",
      "- time_corr: Average p-value = 0.49425\n",
      "  * Lowest p-value observed at: PO3-CP6 (p-value = 0.00274)\n",
      "- spect_corr: Average p-value = 0.55471\n",
      "  * Lowest p-value observed at: C5-F3 (p-value = 0.00064)\n"
     ]
    }
   ],
   "source": [
    "def filter_significant_results(t_test_results, significance_level=0.05):\n",
    "    significant_results = {}\n",
    "    lowest_p_value_features = {}\n",
    "    \n",
    "    for procedure, features in t_test_results.items():\n",
    "        significant_features = {}\n",
    "        lowest_p_values = {}\n",
    "        \n",
    "        for feature, stats in features.items():\n",
    "            feature_name = feature[0]\n",
    "            if feature_name not in significant_features:\n",
    "                significant_features[feature_name] = []\n",
    "                lowest_p_values[feature_name] = (feature[1], stats[\"p_value\"])\n",
    "            else:\n",
    "                if stats[\"p_value\"] < lowest_p_values[feature_name][1]:\n",
    "                    lowest_p_values[feature_name] = (feature[1], stats[\"p_value\"])\n",
    "\n",
    "            # Record p-values regardless of significance\n",
    "            significant_features[feature_name].append(stats[\"p_value\"])\n",
    "        \n",
    "        if significant_features:\n",
    "            significant_results[procedure] = significant_features\n",
    "            lowest_p_value_features[procedure] = lowest_p_values\n",
    "    \n",
    "    return significant_results, lowest_p_value_features\n",
    "\n",
    "def calculate_average_p_values(significant_results):\n",
    "    avg_p_values = {}\n",
    "    \n",
    "    for procedure, features in significant_results.items():\n",
    "        avg_p_values[procedure] = {}\n",
    "        \n",
    "        for feature, p_values in features.items():\n",
    "            if p_values:  # Check to ensure the list is not empty\n",
    "                avg_p_values[procedure][feature] = sum(p_values) / len(p_values)\n",
    "            else:\n",
    "                avg_p_values[procedure][feature] = None  # Or handle as appropriate\n",
    "    \n",
    "    return avg_p_values\n",
    "\n",
    "# Filtering results and finding lowest p-value features\n",
    "significant_results, lowest_p_value_features = filter_significant_results(t_test_results)\n",
    "\n",
    "# Calculating average p-values\n",
    "avg_p_values = calculate_average_p_values(significant_results)\n",
    "\n",
    "# Printing results\n",
    "for procedure, features in avg_p_values.items():\n",
    "    print(f\"Features for {procedure}:\")\n",
    "    for feature, avg_p_value in features.items():\n",
    "        if avg_p_value is not None:  # Ensure there's an average p-value to display\n",
    "            print(f\"- {feature}: Average p-value = {avg_p_value:.5f}\")\n",
    "            lowest_feature_detail, lowest_p_value = lowest_p_value_features[procedure][feature]\n",
    "            print(f\"  * Lowest p-value observed at: {lowest_feature_detail} (p-value = {lowest_p_value:.5f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average p-values for features in itbs:\n",
      "- variance: Average p-value = 0.58940\n",
      "- std: Average p-value = 0.58830\n",
      "- ptp_amp: Average p-value = 0.61729\n",
      "- skewness: Average p-value = 0.54334\n",
      "- kurtosis: Average p-value = 0.62806\n",
      "- rms: Average p-value = 0.59975\n",
      "- hjorth_mobility: Average p-value = 0.29200\n",
      "- hjorth_complexity: Average p-value = 0.27386\n",
      "- zero_crossings: Average p-value = 0.25533\n",
      "- line_length: Average p-value = 0.38005\n",
      "- app_entropy: Average p-value = 0.23917\n",
      "- spect_entropy: Average p-value = 0.30797\n",
      "- hurst_exp: Average p-value = 0.61133\n",
      "- pow_freq_bands: Average p-value = 0.43633\n",
      "- phase_lock_val: Average p-value = 0.46207\n",
      "- time_corr: Average p-value = 0.49236\n",
      "- spect_corr: Average p-value = 0.47147\n",
      "Average p-values for features in ctbs:\n",
      "- variance: Average p-value = 0.47755\n",
      "- std: Average p-value = 0.48018\n",
      "- ptp_amp: Average p-value = 0.50656\n",
      "- skewness: Average p-value = 0.45774\n",
      "- kurtosis: Average p-value = 0.55133\n",
      "- rms: Average p-value = 0.49479\n",
      "- hjorth_mobility: Average p-value = 0.64426\n",
      "- hjorth_complexity: Average p-value = 0.60006\n",
      "- zero_crossings: Average p-value = 0.66166\n",
      "- line_length: Average p-value = 0.64622\n",
      "- app_entropy: Average p-value = 0.59899\n",
      "- spect_entropy: Average p-value = 0.60298\n",
      "- hurst_exp: Average p-value = 0.72349\n",
      "- pow_freq_bands: Average p-value = 0.62856\n",
      "- phase_lock_val: Average p-value = 0.46251\n",
      "- time_corr: Average p-value = 0.48084\n",
      "- spect_corr: Average p-value = 0.55765\n",
      "Average p-values for features in sham:\n",
      "- variance: Average p-value = 0.49939\n",
      "- std: Average p-value = 0.50520\n",
      "- ptp_amp: Average p-value = 0.52391\n",
      "- skewness: Average p-value = 0.21406\n",
      "- kurtosis: Average p-value = 0.54661\n",
      "- rms: Average p-value = 0.48111\n",
      "- hjorth_mobility: Average p-value = 0.64871\n",
      "- hjorth_complexity: Average p-value = 0.65929\n",
      "- zero_crossings: Average p-value = 0.58551\n",
      "- line_length: Average p-value = 0.55078\n",
      "- app_entropy: Average p-value = 0.64605\n",
      "- spect_entropy: Average p-value = 0.66466\n",
      "- hurst_exp: Average p-value = 0.45374\n",
      "- pow_freq_bands: Average p-value = 0.52000\n",
      "- phase_lock_val: Average p-value = 0.49024\n",
      "- time_corr: Average p-value = 0.49425\n",
      "- spect_corr: Average p-value = 0.55471\n"
     ]
    }
   ],
   "source": [
    "def gather_all_p_values(t_test_results):\n",
    "    all_p_values = {}\n",
    "    \n",
    "    for procedure, features in t_test_results.items():\n",
    "        feature_p_values = {}\n",
    "        \n",
    "        for feature, stats in features.items():\n",
    "            feature_name = feature[0]\n",
    "            if feature_name not in feature_p_values:\n",
    "                feature_p_values[feature_name] = []\n",
    "            feature_p_values[feature_name].append(stats[\"p_value\"])\n",
    "        \n",
    "        all_p_values[procedure] = feature_p_values\n",
    "    \n",
    "    return all_p_values\n",
    "\n",
    "all_p_values = gather_all_p_values(t_test_results)\n",
    "avg_p_values = calculate_average_p_values(all_p_values)\n",
    "\n",
    "for procedure, features in avg_p_values.items():\n",
    "    print(f\"Average p-values for features in {procedure}:\")\n",
    "    for feature, avg_p_value in features.items():\n",
    "        print(f\"- {feature}: Average p-value = {avg_p_value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- The most important and recurring features for distinction between the capturing sessions are correlation, spectral power, entropy, and hjorth parameters.\n",
    "- Clear clusters are achieved, but also on sham procedures, which shouldn't result in clusters at all. This likely means that the brain state of the patients varies too much between recording sessions.\n",
    "- Paired t-tests show that the average impact of the procedures on the patients is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
