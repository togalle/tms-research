{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install mne colorlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logger = utils.get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "When the features have been extracted, there's now a list of files containing one entry per epoch. Each file has a certain paradigm containing: patient, procedure, timing. Clustering will be applied for each individual procedure containing all timings (pre & post). These clusters will be generated once with all patients and once per individual patient.\n",
    "\n",
    "The desired outcome is two clusters per plot; one for data before the procedure and one for after the procedure (with the active control procedure being the exception since this should have no impact).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(df, n_clusters=2, plot_title=\"PCA of Clusters\"):\n",
    "    \"\"\"Expects one large dataframe containing all the data to be clustered, and one column 'timings' containing the label for each entry to compare cluster result against ground truth.\"\"\"\n",
    "    if \"timings\" not in df.columns:\n",
    "        logger.error(\"No timings column found in DataFrame\")\n",
    "        return\n",
    "\n",
    "    df_ground_truth = df\n",
    "    df = df.drop(columns=[\"timings\"])\n",
    "\n",
    "    # outlier removal\n",
    "    OUTLIER_THRESHOLD = 0.05\n",
    "    Q1 = df.quantile(0.10)\n",
    "    Q3 = df.quantile(0.90)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    def is_outlier(row):\n",
    "        return ((row < (Q1 - 1.5 * IQR)) | (row > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "    outlier_counts = df.apply(is_outlier, axis=1)\n",
    "    threshold = len(df.columns) * OUTLIER_THRESHOLD\n",
    "    rows_to_drop = outlier_counts[outlier_counts > threshold].index\n",
    "    df_filtered = df.drop(index=rows_to_drop)\n",
    "    print(f\"Original DataFrame shape: {df.shape}\")\n",
    "    print(f\"Filtered DataFrame shape: {df_filtered.shape}\")\n",
    "    df = df_filtered\n",
    "\n",
    "    # normalization\n",
    "    scaler = StandardScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    clusters = kmeans.fit_predict(df)\n",
    "\n",
    "    # # Evaluate the clustering results\n",
    "    # score = metrics.adjusted_rand_score(your_labels, clusters)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    df_reduced = pca.fit_transform(df)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(df_reduced[:, 0], df_reduced[:, 1], c=clusters, cmap=\"viridis\")\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "\n",
    "    print(\n",
    "        f\"lengths: {len(df_reduced[:, 0])}, {len(df_reduced[:, 1])}, {len(df_ground_truth['timings'])}\"\n",
    "    )\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(\n",
    "        df_reduced[:, 0],\n",
    "        df_reduced[:, 1],\n",
    "        c=df_ground_truth[\"timings\"].astype(\"category\").cat.codes,\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "    plt.title(\"KMeans Clustering Results - Ground Truth\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Ground Truth Label\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Attributions\n",
    "    loadings = pca.components_\n",
    "    feature_names = df.columns\n",
    "    for i, component in enumerate(loadings):\n",
    "        component_loadings = zip(feature_names, component)\n",
    "        sorted_loadings = sorted(component_loadings, key=lambda x: abs(x[1]), reverse=True)\n",
    "        print(f\"Principal Component {i+1}:\")\n",
    "        for feature, loading in sorted_loadings[:5]:\n",
    "            print(f\"{feature}: {loading}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = utils.get_metadata_df(\"features\", \"Randomisatielijst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for procedure in [\"itbs\", \"ctbs\", \"sham\"]:\n",
    "    logger.info(f\"Clustering {procedure}\")\n",
    "    # Cluster all patients\n",
    "    filenames = labels[\n",
    "        (labels[\"procedure\"] == procedure)\n",
    "        & (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "    ][\"filename\"]\n",
    "\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "\n",
    "    # Get corresponding labels\n",
    "    pre_post_labels = [\n",
    "        labels[labels[\"filename\"] == file][\"pre_post\"].values[0]\n",
    "        for file in filenames\n",
    "    ]\n",
    "\n",
    "    # Load dataframe\n",
    "    dfs = [\n",
    "        pd.read_csv(os.path.join(\"features\", file), header=[0, 1])\n",
    "        for file in filenames\n",
    "    ]\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Add timings column\n",
    "    ground_truth = []\n",
    "    for timing, df_part in zip(pre_post_labels, dfs):\n",
    "        ground_truth.extend([timing] * len(df_part))\n",
    "    df[\"timings\"] = ground_truth\n",
    "\n",
    "    cluster_df(df, plot_title=procedure)\n",
    "\n",
    "    # Cluster individual patients\n",
    "    for patientid in range(2, 19):\n",
    "        # Get relevant filenames\n",
    "        filenames = labels[\n",
    "            (labels[\"procedure\"] == procedure)\n",
    "            & (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "            & (labels[\"patient_id\"] == f\"{patientid:02}\")\n",
    "        ][\"filename\"]\n",
    "\n",
    "        if len(filenames) == 0:\n",
    "            continue\n",
    "\n",
    "        # Get corresponding labels\n",
    "        pre_post_labels = [\n",
    "            labels[labels[\"filename\"] == file][\"pre_post\"].values[0]\n",
    "            for file in filenames\n",
    "        ]\n",
    "\n",
    "        # Load dataframe\n",
    "        dfs = [\n",
    "            pd.read_csv(os.path.join(\"features\", file), header=[0, 1])\n",
    "            for file in filenames\n",
    "        ]\n",
    "        df = pd.concat(dfs)\n",
    "\n",
    "        # Add timings column\n",
    "        ground_truth = []\n",
    "        for timing, df_part in zip(pre_post_labels, dfs):\n",
    "            ground_truth.extend([timing] * len(df_part))\n",
    "        df[\"timings\"] = ground_truth\n",
    "\n",
    "        cluster_df(df, plot_title=procedure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: Procedure Impact\n",
    "\n",
    "The first clustering is done pre/post procedure. The goal is to see if there is a pattern to the impact a procedure has on all patients.\n",
    "\n",
    "- Patients: all\n",
    "- Procedures: seperate\n",
    "- Timing: all\n",
    "  => 2 or 3 clusters, depending if active control is included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(df, num_clusters):\n",
    "    feature_names = df.columns\n",
    "\n",
    "    # outlier removal\n",
    "    OUTLIER_THRESHOLD = 0.0\n",
    "    Q1 = df.quantile(0.02)\n",
    "    Q3 = df.quantile(0.98)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    def is_outlier(row):\n",
    "        return ((row < (Q1 - 1.5 * IQR)) | (row > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "    outlier_counts = df.apply(is_outlier, axis=1)\n",
    "    threshold = len(df.columns) * OUTLIER_THRESHOLD\n",
    "    rows_to_drop = outlier_counts[outlier_counts > threshold].index\n",
    "    df_filtered = df.drop(index=rows_to_drop)\n",
    "    print(f\"Original DataFrame shape: {df.shape}\")\n",
    "    print(f\"Filtered DataFrame shape: {df_filtered.shape}\")\n",
    "    df = df_filtered\n",
    "\n",
    "    # normalization\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=2)\n",
    "    clusters = kmeans.fit_predict(df)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=num_clusters)\n",
    "    df_reduced = pca.fit_transform(df)\n",
    "\n",
    "    # plot clusters\n",
    "    plt.scatter(df_reduced[:, 0], df_reduced[:, 1], c=clusters, cmap=\"viridis\")\n",
    "    plt.title(\"PCA of Clusters\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "    plt.show()\n",
    "\n",
    "    # Attributions\n",
    "    loadings = pca.components_\n",
    "    for i, component in enumerate(loadings):\n",
    "        component_loadings = zip(feature_names, component)\n",
    "        sorted_loadings = sorted(\n",
    "            component_loadings, key=lambda x: abs(x[1]), reverse=True\n",
    "        )\n",
    "        print(f\"Principal Component {i+1}:\")\n",
    "        for feature, loading in sorted_loadings[:20]:\n",
    "            print(f\"{feature}: {loading}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_procedure_df(procedure_id):\n",
    "    dfs = []\n",
    "    label_df = pd.read_csv(\"labels.csv\")\n",
    "    for filename in os.listdir(\"features\"):\n",
    "        filename = filename.split(\".\")[0]\n",
    "        # get row from label_df with matching filename\n",
    "        row = label_df[label_df[\"filename\"] == filename]\n",
    "        if row[\"procedure\"].values[0] == procedure_id:\n",
    "            df = pd.read_csv(os.path.join(\"features\", filename + \".csv\"), header=[0, 1])\n",
    "            dfs.append(df)\n",
    "    if len(dfs) == 0:\n",
    "        return None\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cTBS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctbs_df = get_procedure_df(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df(ctbs_df, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iTBS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_procedure_df(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
