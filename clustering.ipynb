{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "When the features have been extracted, there's now a list of files containing one entry per epoch. Each file has a certain paradigm containing: patient, procedure, timing. Clustering can be applied in 3 main ways to find patterns in the data:\n",
    "1. **Pre/Post procedure**: For each procedure, check if there are any patterns between pre and post procedure epochs (include all patients, all timings, individual procedures) => 3 graphs\n",
    "2. **cTBS/iTBS/sham**: (all patients, only post, all procedures) => 1 graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "    \"features_test_1.csv\",\n",
    "    \"features_test_2.csv\"\n",
    "]\n",
    "df = pd.concat((pd.read_csv(f, header=[0, 1]) for f in csv_files))\n",
    "\n",
    "# outlier removal\n",
    "# iqr\n",
    "# Q1 = df.quantile(0.25)\n",
    "# Q3 = df.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "# df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "# z-score\n",
    "z_scores = np.abs(stats.zscore(df))\n",
    "df = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "# clustering\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "clusters = kmeans.fit_predict(df)\n",
    "\n",
    "# Plot result with pca\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "df_reduced = pca.fit_transform(df)\n",
    "\n",
    "plt.scatter(df_reduced[:, 0], df_reduced[:, 1], c=clusters, cmap='viridis')\n",
    "plt.title(\"PCA of Clusters\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on long list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = os.listdir(\"features\")\n",
    "\n",
    "# read csv with 2 header rows\n",
    "dfs = [pd.read_csv(os.path.join(\"features\", file), header=[0, 1]) for file in feature_files]\n",
    "df = pd.concat(dfs)\n",
    "feature_names = df.columns\n",
    "\n",
    "# outlier removal\n",
    "OUTLIER_THRESHOLD = 0.10\n",
    "Q1 = df.quantile(0.10)\n",
    "Q3 = df.quantile(0.90)\n",
    "IQR = Q3 - Q1\n",
    "def is_outlier(row):\n",
    "    return ((row < (Q1 - 1.5 * IQR)) | (row > (Q3 + 1.5 * IQR))).sum()\n",
    "outlier_counts = df.apply(is_outlier, axis=1)\n",
    "threshold = len(df.columns) * OUTLIER_THRESHOLD\n",
    "rows_to_drop = outlier_counts[outlier_counts > threshold].index\n",
    "df_filtered = df.drop(index=rows_to_drop)\n",
    "print(f\"Original DataFrame shape: {df.shape}\")\n",
    "print(f\"Filtered DataFrame shape: {df_filtered.shape}\")\n",
    "df = df_filtered\n",
    "\n",
    "# normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "# # Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "clusters = kmeans.fit_predict(df)\n",
    "\n",
    "# # Evaluate the clustering results\n",
    "# score = metrics.adjusted_rand_score(your_labels, clusters)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "df_reduced = pca.fit_transform(df)\n",
    "\n",
    "plt.scatter(df_reduced[:, 0], df_reduced[:, 1], c=clusters, cmap='viridis')\n",
    "plt.title(\"PCA of Clusters\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "# Attributions\n",
    "loadings = pca.components_\n",
    "for i, component in enumerate(loadings):\n",
    "    component_loadings = zip(feature_names, component)\n",
    "    sorted_loadings = sorted(component_loadings, key=lambda x: abs(x[1]), reverse=True)\n",
    "    print(f\"Principal Component {i+1}:\")\n",
    "    for feature, loading in sorted_loadings[:20]:\n",
    "        print(f\"{feature}: {loading}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: Procedure Impact\n",
    "The first clustering is done pre/post procedure. The goal is to see if there is a pattern to the impact a procedure has on all patients.\n",
    "- Patients: all\n",
    "- Procedures: seperate\n",
    "- Timing: all\n",
    "=> 2 or 3 clusters, depending if active control is included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(df, num_clusters):\n",
    "    feature_names = df.columns\n",
    "\n",
    "    # outlier removal\n",
    "    OUTLIER_THRESHOLD = 0.0\n",
    "    Q1 = df.quantile(0.02)\n",
    "    Q3 = df.quantile(0.98)\n",
    "    IQR = Q3 - Q1\n",
    "    def is_outlier(row):\n",
    "        return ((row < (Q1 - 1.5 * IQR)) | (row > (Q3 + 1.5 * IQR))).sum()\n",
    "    outlier_counts = df.apply(is_outlier, axis=1)\n",
    "    threshold = len(df.columns) * OUTLIER_THRESHOLD\n",
    "    rows_to_drop = outlier_counts[outlier_counts > threshold].index\n",
    "    df_filtered = df.drop(index=rows_to_drop)\n",
    "    print(f\"Original DataFrame shape: {df.shape}\")\n",
    "    print(f\"Filtered DataFrame shape: {df_filtered.shape}\")\n",
    "    df = df_filtered\n",
    "\n",
    "    # normalization\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=2)\n",
    "    clusters = kmeans.fit_predict(df)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=num_clusters)\n",
    "    df_reduced = pca.fit_transform(df)\n",
    "\n",
    "    # plot clusters\n",
    "    plt.scatter(df_reduced[:, 0], df_reduced[:, 1], c=clusters, cmap='viridis')\n",
    "    plt.title(\"PCA of Clusters\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Attributions\n",
    "    loadings = pca.components_\n",
    "    for i, component in enumerate(loadings):\n",
    "        component_loadings = zip(feature_names, component)\n",
    "        sorted_loadings = sorted(component_loadings, key=lambda x: abs(x[1]), reverse=True)\n",
    "        print(f\"Principal Component {i+1}:\")\n",
    "        for feature, loading in sorted_loadings[:20]:\n",
    "            print(f\"{feature}: {loading}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_procedure_df(procedure_id):\n",
    "    dfs = []\n",
    "    label_df = pd.read_csv(\"labels.csv\")\n",
    "    for filename in os.listdir(\"features\"):\n",
    "        filename = filename.split(\".\")[0]\n",
    "        # get row from label_df with matching filename\n",
    "        row = label_df[label_df[\"filename\"] == filename]\n",
    "        if(row[\"procedure\"].values[0] == procedure_id):\n",
    "            df = pd.read_csv(os.path.join(\"features\", filename + \".csv\"), header=[0, 1])\n",
    "            dfs.append(df)\n",
    "    if len(dfs) == 0:\n",
    "        return None\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cTBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctbs_df = get_procedure_df(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df(ctbs_df, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iTBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_procedure_df(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
