{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install mne colorlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "logger = utils.get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "When the features have been extracted, there's now a list of files containing one entry per epoch. Each file has a certain paradigm containing: patient, procedure, timing. Clustering will be applied for each individual procedure containing all timings (pre & post). These clusters will be generated once with all patients and once per individual patient.\n",
    "\n",
    "The desired outcome is two clusters per plot; one for data before the procedure and one for after the procedure (with the active control procedure being the exception since this should have no impact).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(df, n_clusters=2, plot_title=\"PCA of Clusters\", algorithm='kmeans'):\n",
    "    \"\"\"Expects one large dataframe containing all the data to be clustered, and one column 'timings' containing the label for each entry to compare cluster result against ground truth.\"\"\"\n",
    "    if \"label\" not in df.columns:\n",
    "        logger.error(\"No timings column found in DataFrame\")\n",
    "        return\n",
    "\n",
    "    df_ground_truth = df\n",
    "    df = df.drop(columns=[\"label\"])\n",
    "    feature_names = df.columns\n",
    "\n",
    "    # Outlier removal\n",
    "    OUTLIER_THRESHOLD = 0.05\n",
    "    Q1 = df.quantile(0.10)\n",
    "    Q3 = df.quantile(0.90)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    def is_outlier(row):\n",
    "        return ((row < (Q1 - 1.5 * IQR)) | (row > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "    outlier_counts = df.apply(is_outlier, axis=1)\n",
    "    threshold = len(df.columns) * OUTLIER_THRESHOLD\n",
    "    rows_to_drop = outlier_counts[outlier_counts > threshold].index\n",
    "    df_filtered = df.drop(index=rows_to_drop)\n",
    "    df_ground_truth = df_ground_truth.drop(index=rows_to_drop)\n",
    "    print(f\"Original DataFrame shape: {df.shape}\")\n",
    "    print(f\"Filtered DataFrame shape: {df_filtered.shape}\")\n",
    "    df = df_filtered\n",
    "\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    # Apply clustering algorithm\n",
    "    if algorithm == 'kmeans':\n",
    "        model = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    elif algorithm == 'gmm':\n",
    "        model = GaussianMixture(n_components=n_clusters, n_init=10)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported algorithm. Use 'kmeans' or 'gmm'.\")\n",
    "\n",
    "    clusters = model.fit_predict(df)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    df_reduced = pca.fit_transform(df)\n",
    "\n",
    "    # 2D Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(df_reduced[:, 0], df_reduced[:, 1], c=clusters, cmap=\"viridis\")\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(\n",
    "        df_reduced[:, 0],\n",
    "        df_reduced[:, 1],\n",
    "        c=df_ground_truth[\"label\"].astype(\"category\").cat.codes,\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "    plt.title(\"KMeans Clustering Results - Ground Truth\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Ground Truth Label\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # 3D Plot\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.scatter(df_reduced[:, 0], df_reduced[:, 1], df_reduced[:, 2], c=clusters, cmap=\"viridis\")\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.scatter(df_reduced[:, 0], df_reduced[:, 1], df_reduced[:, 2], c=df_ground_truth[\"label\"].astype(\"category\").cat.codes, cmap=\"viridis\")\n",
    "    ax.set_title(\"KMeans Clustering Results - Ground Truth\")\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Attributions\n",
    "    loadings = pca.components_\n",
    "    for i, component in enumerate(loadings):\n",
    "        component_loadings = zip(feature_names, component)\n",
    "        sorted_loadings = sorted(component_loadings, key=lambda x: abs(x[1]), reverse=True)\n",
    "        print(f\"Principal Component {i+1}:\")\n",
    "        for feature, loading in sorted_loadings[:5]:\n",
    "            print(f\"{feature}: {loading}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = utils.get_metadata_df(\"features-4\", \"Randomisatielijst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[\"filename\"].to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual procedures\n",
    "\n",
    "The code below applies the clustering function to dataframes for each **individual procedure**. First, it's applied to all patients and then to each individual patient. We expect two seperate clusters for itbs and ctbs: one before and one after procedure, marking the impact of the procedure. Sham shouldn't have any impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for procedure in [\"itbs\", \"ctbs\", \"sham\"]:\n",
    "    logger.info(f\"Clustering {procedure}\")\n",
    "    # Cluster all patients\n",
    "    filenames = labels[\n",
    "        (labels[\"procedure\"] == procedure)\n",
    "        & (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "    ][\"filename\"]\n",
    "\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "\n",
    "    # Get corresponding labels\n",
    "    pre_post_labels = [\n",
    "        labels[labels[\"filename\"] == file][\"pre_post\"].values[0]\n",
    "        for file in filenames\n",
    "    ]\n",
    "\n",
    "    # Load dataframe\n",
    "    dfs = [\n",
    "        pd.read_csv(os.path.join(\"features-4\", file), header=[0, 1])\n",
    "        for file in filenames\n",
    "    ]\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Add timings column\n",
    "    ground_truth = []\n",
    "    for timing, df_part in zip(pre_post_labels, dfs):\n",
    "        ground_truth.extend([timing] * len(df_part))\n",
    "    df[\"label\"] = ground_truth\n",
    "\n",
    "    # cluster_df(df, plot_title=procedure, algorithm=\"gmm\")\n",
    "    cluster_df(df, plot_title=procedure)\n",
    "\n",
    "    # Cluster individual patients\n",
    "    for patientid in range(2, 19):\n",
    "        # Get relevant filenames\n",
    "        filenames = labels[\n",
    "            (labels[\"procedure\"] == procedure)\n",
    "            & (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "            & (labels[\"patient_id\"] == f\"{patientid:02}\")\n",
    "        ][\"filename\"]\n",
    "\n",
    "        if len(filenames) == 0:\n",
    "            continue\n",
    "\n",
    "        # Get corresponding labels\n",
    "        pre_post_labels = [\n",
    "            labels[labels[\"filename\"] == file][\"pre_post\"].values[0]\n",
    "            for file in filenames\n",
    "        ]\n",
    "\n",
    "        # Load dataframe\n",
    "        dfs = [\n",
    "            pd.read_csv(os.path.join(\"features-4\", file), header=[0, 1])\n",
    "            for file in filenames\n",
    "        ]\n",
    "        df = pd.concat(dfs)\n",
    "\n",
    "        # Add timings column\n",
    "        ground_truth = []\n",
    "        for timing, df_part in zip(pre_post_labels, dfs):\n",
    "            ground_truth.extend([timing] * len(df_part))\n",
    "        df[\"label\"] = ground_truth\n",
    "\n",
    "        # cluster_df(df, plot_title=procedure, algorithm=\"gmm\")\n",
    "        cluster_df(df, plot_title=procedure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure differences\n",
    "\n",
    "The code below applies the clustering function to dataframes after **all procedures** (timing is **post**). First, it's applied to all patients and then to each individual patient. We expect three seperate clusters for each procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Clustering all post procedure data\")\n",
    "# Cluster all patients\n",
    "filenames = labels[\n",
    "    (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "    & (labels[\"pre_post\"] == \"post\")\n",
    "][\"filename\"]\n",
    "\n",
    "# Get corresponding labels\n",
    "procedure_labels = [\n",
    "    labels[labels[\"filename\"] == file][\"procedure\"].values[0]\n",
    "    for file in filenames\n",
    "]\n",
    "\n",
    "# Load dataframe\n",
    "dfs = [\n",
    "    pd.read_csv(os.path.join(\"features-4\", file), header=[0, 1])\n",
    "    for file in filenames\n",
    "]\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Add procedure column\n",
    "ground_truth = []\n",
    "for procedure, df_part in zip(procedure_labels, dfs):\n",
    "    ground_truth.extend([procedure] * len(df_part))\n",
    "df[\"label\"] = ground_truth\n",
    "\n",
    "cluster_df(df, n_clusters=3, plot_title=\"Procedure clustering\")\n",
    "\n",
    "# Cluster individual patients\n",
    "for patientid in range(2, 19):\n",
    "    # Get relevant filenames\n",
    "    filenames = labels[\n",
    "        (labels[\"eeg_type\"] == \"rsEEG\")\n",
    "        & (labels[\"patient_id\"] == f\"{patientid:02}\")\n",
    "        & (labels[\"pre_post\"] == \"post\")\n",
    "    ][\"filename\"]\n",
    "\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "\n",
    "    # Get corresponding labels\n",
    "    procedure_labels = [\n",
    "        labels[labels[\"filename\"] == file][\"procedure\"].values[0]\n",
    "        for file in filenames\n",
    "    ]\n",
    "\n",
    "    # Load dataframe\n",
    "    dfs = [\n",
    "        pd.read_csv(os.path.join(\"features-4\", file), header=[0, 1])\n",
    "        for file in filenames\n",
    "    ]\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Add procedure column\n",
    "    ground_truth = []\n",
    "    for procedure, df_part in zip(procedure_labels, dfs):\n",
    "        ground_truth.extend([procedure] * len(df_part))\n",
    "    df[\"label\"] = ground_truth\n",
    "\n",
    "    cluster_df(df, n_clusters=3, plot_title=f\"Procedure clustering patient {patientid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- The most recurring features are correlation, entropy, and hjorth parameters.\n",
    "- Clear clusters are achieved, but also on sham procedures, which shouldn't result in clusters at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
