{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6785343-3bef-4c3b-aac7-b765a12eacbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /opt/conda/lib/python3.11/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (1.14.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (6.8.2)\n",
      "Collecting shap\n",
      "  Downloading shap-0.46.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from mne) (3.1.4)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /opt/conda/lib/python3.11/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from mne) (3.9.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from mne) (24.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /opt/conda/lib/python3.11/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from mne) (4.66.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (71.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.65.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from shap) (2.2.2)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.11/site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.11/site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0->mne) (2.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from pooch>=1.5->mne) (4.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->mne) (2.1.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Downloading shap-0.46.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.2/540.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.46.0 slicer-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mne numpy scipy scikit-learn tensorflow keras colorlog shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c31c2c8-bc22-4eca-9108-52771fde1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-15 15:34:53.601620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-15 15:34:53.651191: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-15 15:34:53.670043: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-15 15:34:53.715312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-15 15:34:56.605461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import mne\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = utils.get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f816da-b8c8-4a23-8703-0646ea74aa1c",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "This notebook explores the usability of neural network architectures on the cleaned rsEEG data. This doesn't make use of the extracted features, which means that there is a larger chance of finding patterns in the raw data and might result in better generalizability to other patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e6b73e-60b0-4956-8c54-1d341ec57f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = utils.get_metadata_df(\"dataset-cleaned\", \"Randomisatielijst.csv\")\n",
    "# patient_ids = [\"02\", \"04\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\"]\n",
    "patient_ids = [\"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\"]\n",
    "train_patients, test_patients = train_test_split(patient_ids, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2454a42e-055c-43ab-b9e3-72ed4c751581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_14_S2_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "274 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_11_S1_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "255 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_18_S3_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_15_S3_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "264 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_14_S2_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "275 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_16_S3_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "213 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_11_S1_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "288 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_10_S2_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "185 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_09_S1_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "249 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_10_S2_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "253 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_12_S1_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "464 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_15_S3_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "271 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_09_S1_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "180 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_12_S1_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "300 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_16_S3_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "247 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_18_S3_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "295 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3792/2350113372.py:7: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  all_epochs = mne.concatenate_epochs(epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "4073 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3792/2350113372.py:10: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  X_1D = all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n"
     ]
    }
   ],
   "source": [
    "filtered_df = labels_df[\n",
    "    (labels_df['eeg_type'] == 'rsEEG')\n",
    "    & (labels_df['procedure'] == 'itbs')\n",
    "    & (labels_df['patient_id'].isin(train_patients))]\n",
    "epoch_files = filtered_df['filename'].tolist()\n",
    "epochs_list = [mne.read_epochs(os.path.join(\"dataset-cleaned\", file)) for file in epoch_files]\n",
    "all_epochs = mne.concatenate_epochs(epochs_list)\n",
    "\n",
    "# Training data & labels\n",
    "X_1D = all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "y_1D = np.array([0 if timing == 'pre' else 1 for timing in filtered_df['pre_post']])\n",
    "\n",
    "# Fix length of data with labels\n",
    "if len(X_1D) != len(y_1D):\n",
    "    y_1D = np.repeat(y_1D, len(X_1D) // len(y_1D) + 1)[:len(X_1D)]\n",
    "if len(X_1D) != len(y_1D):\n",
    "    raise ValueError(f\"Data cardinality is ambiguous: x sizes: {len(X_1D)}, y sizes: {len(y_1D)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff048cf-58a8-4374-8382-8df0485a2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-08-15 15:36:48.698264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10532 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:3b:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723736226.439738   12696 service.cc:146] XLA service 0x7efea0086a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1723736226.439842   12696 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-08-15 15:37:06.592260: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-15 15:37:07.409113: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  4/128\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.4251 - loss: 0.7572 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723736238.073719   12696 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 97ms/step - accuracy: 0.4847 - loss: 0.7900\n",
      "Epoch 2/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.5017 - loss: 0.7064\n",
      "Epoch 3/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5179 - loss: 0.6971\n",
      "Epoch 4/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.4968 - loss: 0.6938\n",
      "Epoch 5/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5188 - loss: 0.6953\n",
      "Epoch 6/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.5348 - loss: 0.6877\n",
      "Epoch 7/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5230 - loss: 0.6864\n",
      "Epoch 8/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5517 - loss: 0.6838\n",
      "Epoch 9/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6062 - loss: 0.6679\n",
      "Epoch 10/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5961 - loss: 0.6609\n",
      "Epoch 11/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6192 - loss: 0.6501\n",
      "Epoch 12/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.6691 - loss: 0.6280\n",
      "Epoch 13/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6528 - loss: 0.6241\n",
      "Epoch 14/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6747 - loss: 0.5944\n",
      "Epoch 15/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.6766 - loss: 0.5955\n",
      "Epoch 16/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6779 - loss: 0.5851\n",
      "Epoch 17/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.6708 - loss: 0.5907\n",
      "Epoch 18/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7082 - loss: 0.5661\n",
      "Epoch 19/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7062 - loss: 0.5586\n",
      "Epoch 20/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7301 - loss: 0.5322\n",
      "Epoch 21/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7347 - loss: 0.5322\n",
      "Epoch 22/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7339 - loss: 0.5287\n",
      "Epoch 23/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7521 - loss: 0.4950\n",
      "Epoch 24/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7358 - loss: 0.5189\n",
      "Epoch 25/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7465 - loss: 0.4937\n",
      "Epoch 26/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7526 - loss: 0.4921\n",
      "Epoch 27/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7746 - loss: 0.4643\n",
      "Epoch 28/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7637 - loss: 0.4758\n",
      "Epoch 29/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7750 - loss: 0.4635\n",
      "Epoch 30/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7896 - loss: 0.4418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f0126181050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1D = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_1D.shape[1], X_1D.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv1D(256, 3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_1D.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_1D.fit(X_1D, y_1D, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82619a24-d280-45fe-a82f-e9f9bdceaa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-15 15:38:33,741] - INFO - Testing on patient 13\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_13_S2_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "298 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_13_S2_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "346 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3792/398768274.py:7: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "644 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3792/398768274.py:9: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  X_test = new_all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-15 15:38:41,453] - INFO - Accuracy for patient 13: 0.4829192546583851\u001b[0m\n",
      "\u001b[32m[2024-08-15 15:38:41,454] - INFO - Testing on patient 08\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_08_S2_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "240 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_08_S2_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "317 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3792/398768274.py:7: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "557 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3792/398768274.py:9: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  X_test = new_all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-15 15:38:47,272] - INFO - Accuracy for patient 08: 0.4919210053859964\u001b[0m\n",
      "\u001b[32m[2024-08-15 15:38:47,273] - INFO - Testing on patient 17\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_17_S1_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "300 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_17_S1_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "258 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3792/398768274.py:7: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "558 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3792/398768274.py:9: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  X_test = new_all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-15 15:38:54,067] - INFO - Accuracy for patient 17: 0.34587813620071683\u001b[0m\n",
      "\u001b[32m[2024-08-15 15:38:54,070] - INFO - Total accuracy (averaged): 0.4402394654150328\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "total_accuracy = []\n",
    "for new_patient_id in test_patients:\n",
    "    logger.info(f\"Testing on patient {new_patient_id}\")\n",
    "    new_filtered_df = labels_df[(labels_df['procedure'] == 'itbs') & (labels_df['eeg_type'] == 'rsEEG') & (labels_df['patient_id'] == new_patient_id)]\n",
    "    new_epoch_files = new_filtered_df['filename'].tolist()\n",
    "    new_epochs_list = [mne.read_epochs(os.path.join(\"dataset-cleaned\", file)) for file in new_epoch_files]\n",
    "    new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n",
    "    \n",
    "    X_test = new_all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "    y_test = np.array([0 if timing == 'pre' else 1 for timing in new_filtered_df['pre_post']])\n",
    "    \n",
    "    # Ensure y has the same number of samples as X\n",
    "    if len(X_test) != len(y_test):\n",
    "        y_test = np.repeat(y_test, len(X_test) // len(y_test) + 1)[:len(X_test)]\n",
    "    \n",
    "    # Check if the lengths match\n",
    "    if len(X_test) != len(y_test):\n",
    "        raise ValueError(f\"Data cardinality is ambiguous: x sizes: {len(X_test)}, y sizes: {len(y_test)}\")\n",
    "    \n",
    "    y_pred = model_1D.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logger.info(f'Accuracy for patient {new_patient_id}: {accuracy}')\n",
    "    total_accuracy.append(accuracy)\n",
    "\n",
    "average_accuracy = np.mean(total_accuracy)\n",
    "logger.info(f'Total accuracy (averaged): {average_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5ca6a-dbe2-4829-9d12-30dfb04cad83",
   "metadata": {},
   "source": [
    "## 2D Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3ef67a-3289-4836-95a5-4e89b41b33ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-08-14 13:06:09.492002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10532 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2024-08-14 13:06:19.746456: W external/local_tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.32GiB (rounded to 7860387840)requested by op Mul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-08-14 13:06:19.746525: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2024-08-14 13:06:19.746552: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 18, Chunks in use: 18. 4.5KiB allocated for chunks. 4.5KiB in use in bin. 1.3KiB client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746583: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 5, Chunks in use: 5. 2.5KiB allocated for chunks. 2.5KiB in use in bin. 2.5KiB client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746595: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 2, Chunks in use: 2. 2.2KiB allocated for chunks. 2.2KiB in use in bin. 2.0KiB client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746607: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746619: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746630: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746641: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746652: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746663: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746678: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 2, Chunks in use: 1. 381.0KiB allocated for chunks. 128.0KiB in use in bin. 128.0KiB client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746691: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746733: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746767: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746787: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746806: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746825: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746855: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746874: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746893: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746928: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-14 13:06:19.746951: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 1. 10.28GiB allocated for chunks. 7.32GiB in use in bin. 7.32GiB client-requested in use in bin.\n",
      "2024-08-14 13:06:19.747013: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 7.32GiB was 256.00MiB, Chunk State: \n",
      "2024-08-14 13:06:19.747044: I external/local_tsl/tsl/framework/bfc_allocator.cc:1068]   Size: 2.96GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 7.32GiB | Requested Size: 7.32GiB | in_use: 1 | bin_num: -1\n",
      "2024-08-14 13:06:19.747064: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 11043733504\n",
      "2024-08-14 13:06:19.747082: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000000 of size 256 next 1\n",
      "2024-08-14 13:06:19.747092: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000100 of size 1280 next 2\n",
      "2024-08-14 13:06:19.747102: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000600 of size 256 next 3\n",
      "2024-08-14 13:06:19.747111: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000700 of size 256 next 4\n",
      "2024-08-14 13:06:19.747120: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000800 of size 256 next 5\n",
      "2024-08-14 13:06:19.747129: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000900 of size 256 next 7\n",
      "2024-08-14 13:06:19.747138: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000a00 of size 256 next 8\n",
      "2024-08-14 13:06:19.747147: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000b00 of size 256 next 6\n",
      "2024-08-14 13:06:19.747156: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000c00 of size 256 next 9\n",
      "2024-08-14 13:06:19.747165: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000d00 of size 256 next 12\n",
      "2024-08-14 13:06:19.747175: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000e00 of size 256 next 13\n",
      "2024-08-14 13:06:19.747184: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0000f00 of size 256 next 14\n",
      "2024-08-14 13:06:19.747193: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0001000 of size 256 next 15\n",
      "2024-08-14 13:06:19.747202: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0001100 of size 256 next 16\n",
      "2024-08-14 13:06:19.747211: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0001200 of size 512 next 10\n",
      "2024-08-14 13:06:19.747222: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0001400 of size 1024 next 11\n",
      "2024-08-14 13:06:19.747231: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0001800 of size 512 next 18\n",
      "2024-08-14 13:06:19.747240: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0001a00 of size 512 next 17\n",
      "2024-08-14 13:06:19.747249: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0001c00 of size 512 next 19\n",
      "2024-08-14 13:06:19.747258: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0001e00 of size 512 next 22\n",
      "2024-08-14 13:06:19.747267: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0002000 of size 256 next 23\n",
      "2024-08-14 13:06:19.747289: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0002100 of size 256 next 24\n",
      "2024-08-14 13:06:19.747298: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0002200 of size 256 next 25\n",
      "2024-08-14 13:06:19.747307: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0002300 of size 256 next 26\n",
      "2024-08-14 13:06:19.747317: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0002400 of size 256 next 28\n",
      "2024-08-14 13:06:19.747326: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7faba0002500 of size 259072 next 20\n",
      "2024-08-14 13:06:19.747335: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0041900 of size 131072 next 21\n",
      "2024-08-14 13:06:19.747357: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7faba0061900 of size 7860387840 next 27\n",
      "2024-08-14 13:06:19.747366: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7fad748a1900 of size 3182946048 next 18446744073709551615\n",
      "2024-08-14 13:06:19.747376: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2024-08-14 13:06:19.747388: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 18 Chunks of size 256 totalling 4.5KiB\n",
      "2024-08-14 13:06:19.747399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 512 totalling 2.5KiB\n",
      "2024-08-14 13:06:19.747409: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1024 totalling 1.0KiB\n",
      "2024-08-14 13:06:19.747419: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-08-14 13:06:19.747430: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 131072 totalling 128.0KiB\n",
      "2024-08-14 13:06:19.747441: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 7860387840 totalling 7.32GiB\n",
      "2024-08-14 13:06:19.747451: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 7.32GiB\n",
      "2024-08-14 13:06:19.747462: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 11043733504 memory_limit_: 11043733504 available bytes: 0 curr_region_allocation_bytes_: 22087467008\n",
      "2024-08-14 13:06:19.747475: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                     11043733504\n",
      "InUse:                      7860528384\n",
      "MaxInUse:                   7860528384\n",
      "NumAllocs:                          39\n",
      "MaxAllocSize:               7860387840\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-08-14 13:06:19.747488: W external/local_tsl/tsl/framework/bfc_allocator.cc:494] ************************************************************************____________________________\n",
      "2024-08-14 13:06:19.747506: W tensorflow/core/framework/op_kernel.cc:1828] RESOURCE_EXHAUSTED: failed to allocate memory\n",
      "2024-08-14 13:06:19.747527: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m y_2D \u001b[38;5;241m=\u001b[39m y_1D\n\u001b[1;32m      5\u001b[0m X_CNN \u001b[38;5;241m=\u001b[39m X_2D\u001b[38;5;241m.\u001b[39mreshape(X_2D\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_2D\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_2D\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (n_epochs, n_channels, n_times, 1)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model_CNN \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_CNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_CNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_CNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBatchNormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#MaxPooling2D((2, 2)),\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#Dropout(0.25),\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBatchNormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#MaxPooling2D((2, 2)),\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#Dropout(0.25),\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m model_CNN\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/models/sequential.py:74\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, layers, trainable, name)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(layer, rebuild\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/models/sequential.py:139\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    138\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/layers/layer.py:225\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    224\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 225\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/models/sequential.py:183\u001b[0m, in \u001b[0;36mSequential.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "# 2x2 and 3x3 kernel have same result\n",
    "\n",
    "X_2D = X_1D\n",
    "y_2D = y_1D\n",
    "X_CNN = X_2D.reshape(X_2D.shape[0], X_2D.shape[1], X_2D.shape[2], 1)  # Shape: (n_epochs, n_channels, n_times, 1)\n",
    "\n",
    "model_CNN = Sequential([\n",
    "    Conv2D(64, (2, 2), activation='relu', input_shape=(X_CNN.shape[1], X_CNN.shape[2], X_CNN.shape[3])),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling2D((2, 2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Conv2D(128, (2, 2), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling2D((2, 2)),\n",
    "    #Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_CNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_CNN.fit(X_CNN, y_2D, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5f86d-c0e7-4513-9e51-63357cd5aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy = []\n",
    "for new_patient_id in test_patients:\n",
    "    logger.info(f\"Testing on patient {new_patient_id}\")\n",
    "    new_filtered_df = labels_df[(labels_df['procedure'] == 'itbs') & (labels_df['eeg_type'] == 'rsEEG') & (labels_df['patient_id'] == new_patient_id)]\n",
    "    new_epoch_files = new_filtered_df['filename'].tolist()\n",
    "    new_epochs_list = [mne.read_epochs(os.path.join(\"dataset-cleaned\", file)) for file in new_epoch_files]\n",
    "    new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n",
    "    \n",
    "    X_test = new_all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "    y_test = np.array([0 if timing == 'pre' else 1 for timing in new_filtered_df['pre_post']])\n",
    "    \n",
    "    # Ensure y has the same number of samples as X\n",
    "    if len(X_test) != len(y_test):\n",
    "        y_test = np.repeat(y_test, len(X_test) // len(y_test) + 1)[:len(X_test)]\n",
    "    \n",
    "    # Check if the lengths match\n",
    "    if len(X_test) != len(y_test):\n",
    "        raise ValueError(f\"Data cardinality is ambiguous: x sizes: {len(X_test)}, y sizes: {len(y_test)}\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logger.info(f'Accuracy for patient {new_patient_id}: {accuracy}')\n",
    "    total_accuracy.append(accuracy)\n",
    "\n",
    "average_accuracy = np.mean(total_accuracy)\n",
    "logger.info(f'Total accuracy (averaged): {average_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52faf76-02e8-4e4f-a044-2107b79d7958",
   "metadata": {},
   "source": [
    "## XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5d332-21dd-48d2-91a7-990a0a1b654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "explainer = shap.DeepExplainer(1D_model, X_1D[:100])\n",
    "shap_values = explainer.shap_values(X_test[:10])\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "\n",
    "total_accuracy = []\n",
    "for new_patient_id in test_patients:\n",
    "    logger.info(f\"Testing on patient {new_patient_id}\")\n",
    "    new_filtered_df = labels_df[(labels_df['procedure'] == 'itbs') & (labels_df['eeg_type'] == 'rsEEG') & (labels_df['patient_id'] == new_patient_id)]\n",
    "    new_epoch_files = new_filtered_df['filename'].tolist()\n",
    "    new_epochs_list = [mne.read_epochs(os.path.join(\"dataset-cleaned\", file)) for file in new_epoch_files]\n",
    "    new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n",
    "    \n",
    "    X_test = new_all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "    y_test = np.array([0 if timing == 'pre' else 1 for timing in new_filtered_df['pre_post']])\n",
    "    \n",
    "    # Ensure y has the same number of samples as X\n",
    "    if len(X_test) != len(y_test):\n",
    "        y_test = np.repeat(y_test, len(X_test) // len(y_test) + 1)[:len(X_test)]\n",
    "    \n",
    "    # Check if the lengths match\n",
    "    if len(X_test) != len(y_test):\n",
    "        raise ValueError(f\"Data cardinality is ambiguous: x sizes: {len(X_test)}, y sizes: {len(y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
