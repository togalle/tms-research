{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6785343-3bef-4c3b-aac7-b765a12eacbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /opt/conda/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (1.14.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.11/site-packages (3.4.1)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting shap\n",
      "  Downloading shap-0.46.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from mne) (3.1.4)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /opt/conda/lib/python3.11/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /opt/conda/lib/python3.11/site-packages (from mne) (3.9.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from mne) (24.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /opt/conda/lib/python3.11/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from mne) (4.66.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (71.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.65.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from shap) (2.2.2)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.11/site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.11/site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.6->mne) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.6->mne) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.6->mne) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.6->mne) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.6->mne) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.6->mne) (2.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from pooch>=1.5->mne) (4.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->mne) (2.1.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading shap-0.46.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.2/540.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: slicer, colorlog, shap\n",
      "Successfully installed colorlog-6.8.2 shap-0.46.0 slicer-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mne numpy scipy scikit-learn tensorflow keras colorlog shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c31c2c8-bc22-4eca-9108-52771fde1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 14:08:32.525914: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-24 14:08:32.907762: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-24 14:08:32.996931: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-24 14:08:33.803945: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-24 14:08:36.519073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import mne\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "import os\n",
    "import shap\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = utils.get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f816da-b8c8-4a23-8703-0646ea74aa1c",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "This notebook explores the usability of neural network architectures on the cleaned rsEEG data. This doesn't make use of the extracted features, which means that there is a larger chance of finding patterns in the raw data and might result in better generalizability to other patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e6b73e-60b0-4956-8c54-1d341ec57f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = utils.get_metadata_df(\"dataset-cleaned\", \"Randomisatielijst.csv\")\n",
    "# patient_ids = [\"02\", \"04\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\"]\n",
    "patient_ids = [\"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\"]\n",
    "train_patients, test_patients = train_test_split(patient_ids, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2454a42e-055c-43ab-b9e3-72ed4c751581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_14_S2_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "274 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_11_S1_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "255 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_18_S3_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_15_S3_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "264 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_14_S2_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "275 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_16_S3_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "213 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_11_S1_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "288 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_10_S2_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "185 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_09_S1_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "249 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_10_S2_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "253 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_12_S1_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "464 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_15_S3_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "271 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_09_S1_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "180 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_12_S1_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "300 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_16_S3_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "247 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_18_S3_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "295 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11352/2350113372.py:7: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  all_epochs = mne.concatenate_epochs(epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "4073 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    }
   ],
   "source": [
    "filtered_df = labels_df[\n",
    "    (labels_df['eeg_type'] == 'rsEEG')\n",
    "    & (labels_df['procedure'] == 'itbs')\n",
    "    & (labels_df['patient_id'].isin(train_patients))]\n",
    "epoch_files = filtered_df['filename'].tolist()\n",
    "epochs_list = [mne.read_epochs(os.path.join(\"dataset-cleaned\", file)) for file in epoch_files]\n",
    "all_epochs = mne.concatenate_epochs(epochs_list)\n",
    "\n",
    "# Training data & labels\n",
    "X_1D = all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "y_1D = np.array([0 if timing == 'pre' else 1 for timing in filtered_df['pre_post']])\n",
    "\n",
    "# Fix length of data with labels\n",
    "if len(X_1D) != len(y_1D):\n",
    "    y_1D = np.repeat(y_1D, len(X_1D) // len(y_1D) + 1)[:len(X_1D)]\n",
    "if len(X_1D) != len(y_1D):\n",
    "    raise ValueError(f\"Data cardinality is ambiguous: x sizes: {len(X_1D)}, y sizes: {len(y_1D)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff048cf-58a8-4374-8382-8df0485a2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-08-24 14:09:27.784507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10532 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "2024-08-24 14:09:40.333623: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2021218104 exceeds 10% of free system memory.\n",
      "2024-08-24 14:09:52.741940: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2021218104 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724508607.251577   11565 service.cc:146] XLA service 0x7f28dc088ad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1724508607.252358   11565 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-08-24 14:10:07.973775: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-24 14:10:09.368548: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 16/128\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5140 - loss: 0.8267"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1724508618.773136   11565 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - accuracy: 0.5007 - loss: 0.8176\n",
      "Epoch 2/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5204 - loss: 0.6961\n",
      "Epoch 3/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4991 - loss: 0.6981\n",
      "Epoch 4/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5250 - loss: 0.6954\n",
      "Epoch 5/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5456 - loss: 0.6888\n",
      "Epoch 6/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5478 - loss: 0.6847\n",
      "Epoch 7/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5674 - loss: 0.6757\n",
      "Epoch 8/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6079 - loss: 0.6695\n",
      "Epoch 9/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6242 - loss: 0.6529\n",
      "Epoch 10/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6220 - loss: 0.6432\n",
      "Epoch 11/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6599 - loss: 0.6188\n",
      "Epoch 12/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.6449 - loss: 0.6183\n",
      "Epoch 13/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6729 - loss: 0.5972\n",
      "Epoch 14/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6874 - loss: 0.5867\n",
      "Epoch 15/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6820 - loss: 0.5826\n",
      "Epoch 16/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6906 - loss: 0.5692\n",
      "Epoch 17/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6986 - loss: 0.5752\n",
      "Epoch 18/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7160 - loss: 0.5506\n",
      "Epoch 19/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7207 - loss: 0.5398\n",
      "Epoch 20/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7379 - loss: 0.5293\n",
      "Epoch 21/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7394 - loss: 0.5229\n",
      "Epoch 22/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7513 - loss: 0.5100\n",
      "Epoch 23/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7441 - loss: 0.4979\n",
      "Epoch 24/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7662 - loss: 0.4815\n",
      "Epoch 25/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7582 - loss: 0.4782\n",
      "Epoch 26/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7669 - loss: 0.4656\n",
      "Epoch 27/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7743 - loss: 0.4460\n",
      "Epoch 28/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7870 - loss: 0.4354\n",
      "Epoch 29/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8028 - loss: 0.4254\n",
      "Epoch 30/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8038 - loss: 0.3991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f2c6a47ced0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1D = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_1D.shape[1], X_1D.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv1D(256, 3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_1D.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_1D.fit(X_1D, y_1D, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82619a24-d280-45fe-a82f-e9f9bdceaa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-24 16:40:10,177] - INFO - Testing on patient 13\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_13_S2_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "298 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_13_S2_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "346 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11352/3669168602.py:22: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "644 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 16:40:30.435681: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 319583712 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-24 16:40:32,647] - INFO - Accuracy for patient 13: 0.5015527950310559\u001b[0m\n",
      "\u001b[32m[2024-08-24 16:40:32,648] - INFO - Testing on patient 08\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_08_S2_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "240 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_08_S2_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "317 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11352/3669168602.py:22: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "557 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-24 16:40:51,812] - INFO - Accuracy for patient 08: 0.49730700179533216\u001b[0m\n",
      "\u001b[32m[2024-08-24 16:40:51,813] - INFO - Testing on patient 17\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_17_S1_rsEEG_pre-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "300 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /project_ghent/Tomas_research/tms-research/dataset-cleaned/TMS-EEG-H_17_S1_rsEEG_post-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "258 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11352/3669168602.py:22: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "558 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-24 16:41:08,621] - INFO - Accuracy for patient 17: 0.4014336917562724\u001b[0m\n",
      "\u001b[32m[2024-08-24 16:41:08,623] - INFO - Total accuracy (averaged): 0.46676449619422017\u001b[0m\n",
      "\u001b[32m[2024-08-24 16:41:08,623] - INFO - Margin of error: 0.0640692411427246\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Currently, SHAP results in ResourceExhaustedError\n",
    "USE_SHAP = False\n",
    "\n",
    "if USE_SHAP:\n",
    "    explainer = shap.DeepExplainer(model_1D, X_1D)\n",
    "    shap_values_list = []\n",
    "\n",
    "total_accuracy = []\n",
    "\n",
    "for new_patient_id in test_patients:\n",
    "    logger.info(f\"Testing on patient {new_patient_id}\")\n",
    "    \n",
    "    new_filtered_df = labels_df[\n",
    "        (labels_df['procedure'] == 'itbs') \n",
    "        & (labels_df['eeg_type'] == 'rsEEG') \n",
    "        & (labels_df['patient_id'] == new_patient_id)\n",
    "    ]\n",
    "    new_epoch_files = new_filtered_df['filename'].tolist()\n",
    "    new_epochs_list = [mne.read_epochs(os.path.join(\"dataset-cleaned\", file)) for file in new_epoch_files]\n",
    "    new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n",
    "    \n",
    "    X_test = new_all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "    y_test = np.array([0 if timing == 'pre' else 1 for timing in new_filtered_df['pre_post']])\n",
    "    \n",
    "    if len(X_test) != len(y_test):\n",
    "        y_test = np.repeat(y_test, len(X_test) // len(y_test) + 1)[:len(X_test)]\n",
    "    \n",
    "    if len(X_test) != len(y_test):\n",
    "        raise ValueError(f\"Data cardinality is ambiguous: x sizes: {len(X_test)}, y sizes: {len(y_test)}\")\n",
    "    \n",
    "    y_pred = model_1D.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5).astype(int).flatten()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logger.info(f'Accuracy for patient {new_patient_id}: {accuracy}')\n",
    "    total_accuracy.append(accuracy)\n",
    "    \n",
    "    if USE_SHAP:\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap_values_list.append(shap_values)\n",
    "\n",
    "average_accuracy = np.mean(total_accuracy)\n",
    "sem = stats.sem(total_accuracy)\n",
    "margin_of_error = 1.96 * sem\n",
    "\n",
    "logger.info(f'Total accuracy (averaged): {average_accuracy}')\n",
    "logger.info(f'Margin of error: {margin_of_error}')\n",
    "\n",
    "if USE_SHAP:\n",
    "    combined_shap_values = np.concatenate(shap_values_list, axis=0)\n",
    "    shap.summary_plot(combined_shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c00fd0-6630-4320-9d9e-b51378d63f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-08-24 16:41:08,630] - INFO - Total accuracy (averaged): 0.46676449619422017\u001b[0m\n",
      "\u001b[32m[2024-08-24 16:41:08,633] - INFO - Margin of error: 0.0640692411427246\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "average_accuracy = np.mean(total_accuracy)\n",
    "sem = stats.sem(total_accuracy)\n",
    "margin_of_error = 1.96 * sem\n",
    "\n",
    "logger.info(f'Total accuracy (averaged): {average_accuracy}')\n",
    "logger.info(f'Margin of error: {margin_of_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0504f9d-e60b-4600-92ef-943bdec37386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80d5ca6a-dbe2-4829-9d12-30dfb04cad83",
   "metadata": {},
   "source": [
    "## 2D Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa3ef67a-3289-4836-95a5-4e89b41b33ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 15:20:20.623119: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2021218104 exceeds 10% of free system memory.\n",
      "2024-08-24 15:20:28.238254: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2021218104 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 173ms/step - accuracy: 0.4903 - loss: 0.8742\n",
      "Epoch 2/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 131ms/step - accuracy: 0.5082 - loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 130ms/step - accuracy: 0.4935 - loss: 0.6932\n",
      "Epoch 4/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 131ms/step - accuracy: 0.5001 - loss: 0.6932\n",
      "Epoch 5/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 131ms/step - accuracy: 0.4910 - loss: 0.6932\n",
      "Epoch 6/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - accuracy: 0.5058 - loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 131ms/step - accuracy: 0.4856 - loss: 0.6932\n",
      "Epoch 8/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 131ms/step - accuracy: 0.5000 - loss: 0.6932\n",
      "Epoch 9/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - accuracy: 0.4916 - loss: 0.6932\n",
      "Epoch 10/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - accuracy: 0.4967 - loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f2c69de1e10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2D = X_1D\n",
    "y_2D = y_1D\n",
    "X_CNN = X_2D.reshape(X_2D.shape[0], X_2D.shape[1], X_2D.shape[2], 1)\n",
    "\n",
    "model_CNN = Sequential([\n",
    "    Conv2D(64, (2, 2), activation='relu', input_shape=(X_CNN.shape[1], X_CNN.shape[2], X_CNN.shape[3])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(128, (2, 2), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_CNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_CNN.fit(X_CNN, y_2D, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5f86d-c0e7-4513-9e51-63357cd5aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, X_1D)\n",
    "shap_values_list = []\n",
    "total_accuracy = []\n",
    "\n",
    "for new_patient_id in test_patients:\n",
    "    logger.info(f\"Testing on patient {new_patient_id}\")\n",
    "    new_filtered_df = labels_df[(labels_df['procedure'] == 'itbs') & (labels_df['eeg_type'] == 'rsEEG') & (labels_df['patient_id'] == new_patient_id)]\n",
    "    new_epoch_files = new_filtered_df['filename'].tolist()\n",
    "    new_epochs_list = [mne.read_epochs(os.path.join(\"dataset-cleaned\", file)) for file in new_epoch_files]\n",
    "    new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n",
    "    \n",
    "    X_test = new_all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "    y_test = np.array([0 if timing == 'pre' else 1 for timing in new_filtered_df['pre_post']])\n",
    "    \n",
    "    # Ensure y has the same number of samples as X\n",
    "    if len(X_test) != len(y_test):\n",
    "        y_test = np.repeat(y_test, len(X_test) // len(y_test) + 1)[:len(X_test)]\n",
    "    \n",
    "    # Check if the lengths match\n",
    "    if len(X_test) != len(y_test):\n",
    "        raise ValueError(f\"Data cardinality is ambiguous: x sizes: {len(X_test)}, y sizes: {len(y_test)}\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5).astype(int).flatten()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logger.info(f'Accuracy for patient {new_patient_id}: {accuracy}')\n",
    "    total_accuracy.append(accuracy)\n",
    "\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap_values_list.append(shap_values)\n",
    "\n",
    "average_accuracy = np.mean(total_accuracy)\n",
    "logger.info(f'Total accuracy (averaged): {average_accuracy}')\n",
    "\n",
    "combined_shap_values = np.concatenate(shap_values_list, axis=0)\n",
    "shap.summary_plot(combined_shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52faf76-02e8-4e4f-a044-2107b79d7958",
   "metadata": {},
   "source": [
    "## XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e5d332-21dd-48d2-91a7-990a0a1b654f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/shap/explainers/_deep/deep_tf.py:99: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\"Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\")\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1723752777.443875     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.477465     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.478860     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.480710     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.482085     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.483750     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.485554     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.488094     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.489782     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.493408     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.494975     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.496766     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.498670     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.501381     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.504150     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.847952     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.848857     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.849582     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.850276     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.850956     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.851587     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.852208     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.852866     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.853687     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.854311     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.854924     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.855560     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.856210     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.856855     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.857480     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.858067     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.889306     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.890280     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.890974     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.891697     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.892499     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.893231     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.893932     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.894701     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.895914     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.896744     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.897527     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.898270     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.899062     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.899801     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.900534     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1723752777.901223     584 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model_1D, X_1D[:\u001b[38;5;241m100\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(\u001b[43mX_test\u001b[49m[:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m      6\u001b[0m shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values, X_test)\n\u001b[1;32m      9\u001b[0m total_accuracy \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "explainer = shap.DeepExplainer(model_1D, X_1D[:100])\n",
    "shap_values = explainer.shap_values(X_test[:10])\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "\n",
    "total_accuracy = []\n",
    "for new_patient_id in test_patients:\n",
    "    logger.info(f\"Testing on patient {new_patient_id}\")\n",
    "    new_filtered_df = labels_df[(labels_df['procedure'] == 'itbs') & (labels_df['eeg_type'] == 'rsEEG') & (labels_df['patient_id'] == new_patient_id)]\n",
    "    new_epoch_files = new_filtered_df['filename'].tolist()\n",
    "    new_epochs_list = [mne.read_epochs(os.path.join(\"dataset-cleaned\", file)) for file in new_epoch_files]\n",
    "    new_all_epochs = mne.concatenate_epochs(new_epochs_list)\n",
    "    \n",
    "    X_test = new_all_epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "    y_test = np.array([0 if timing == 'pre' else 1 for timing in new_filtered_df['pre_post']])\n",
    "    \n",
    "    # Ensure y has the same number of samples as X\n",
    "    if len(X_test) != len(y_test):\n",
    "        y_test = np.repeat(y_test, len(X_test) // len(y_test) + 1)[:len(X_test)]\n",
    "    \n",
    "    # Check if the lengths match\n",
    "    if len(X_test) != len(y_test):\n",
    "        raise ValueError(f\"Data cardinality is ambiguous: x sizes: {len(X_test)}, y sizes: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f620f4c-b02c-4711-81d7-899b0a2faf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
